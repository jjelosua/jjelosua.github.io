{"pages":[{"text":"I am a Spanish Telecommunications engineer with broad experience in tech consultancy and financial services IT. In 2011, I discovered data journalism and became a data addict freelance developer. I love turning data upside down and inside out, trying to extract knowledge out of it. More comfortable with backend tasks, but also a big D3 fan, which I have used to create some nice interactive visualizations. In Spain we did not have a \"freedom of information access\" law until December 2014, so I have been writing scraper scripts to unlock government data for a while, in order to make the information available in a more comprehensive format to citizens. To do so I enjoy working with Python ( Requests and Beautifulsoup ), Ruby ( Mechanize and Nokogiri ) and recently with Phamtomjs to work around ajax sites. I have also trained journalists in Spain and abroad in tools that allow them to find stories and work more efficiently in their data related projects. I strongly believe that open data, transparency and accountability will play a key role in shaping the future of modern societies. When not around my computer you can find me travelling, having a looong conversation with friends or swimming in the ocean. 2015 is going to be a great year!! I was selected as a 2015 Knight-Mozilla fellow ( check out the program here ). I will join La Nacion Data Team to work on cool data journalism projects. This blog has been set up to share my experience throughout this year, please feel free to contact me through social media or by email","tags":"pages","loc":"http://www.juanelosua.com/about.html","title":"About"},{"text":"Selection of the projects that I have worked on over the past couple of years. In most of them I have worked with civio , an spanish non-profit that promotes transparency, accountability and free access to public data to fight for a stronger democracy and empower citizens to have a more proactive role in modern society. España en Llamas (Spanish) Description First Data Journalism project that I have worked on (Hooray!!!). It started with a FOIA request to the spanish government to release the data for every individual forest fires that had occur in Spain from 2001 onwards. After many data wrangling tasks, we achieved a clean database with all the geolocated forest fires with all the available metadata for each one of them. We then created a site that covers some interesting stories we found in the data as well as an interactive map that allows readers to focus on the fires that are more keen to them. Technologies Backend: GIS related tools to transform coordinates, DB transformations Frontend: Google Maps API & D3.js for the interactive map El Indultómetro (Spanish) Description Project also developed in collaboration with civio that compiles, analyzes and classifies all the information in the spanish official gazette about pardons since 1996. Let's readers search by the type of offense, compare annual data and determine how each party in the government has used this controversial measure. Technologies Backend: Ruby mechanize and nokogiri for data collection, Sinatra for the site Frontend: HTML5, CSS3, d3.js, Timeline.js for the visualizations Donde van mis impuestos (Spanish) Description Side project integrated inside a bigger site developed by civio that breaks down spanish budget expenditures. The module compares the regional budget expenditures in Spain from 2006 onwards. Technologies Backend: Ruby gems mechanize & nokogiri to build the scraper Frontend: HTML5, CSS3, D3.js for the visualization Congreso ¿quién es quien? (Spanish) Description Project developed during the 2014 International Open Data Day together with Javier Sanroman and Diego Pino . We gather data for every spanish member of the parliament since modern democracy (1977) together with some metadata related to their activities inside the congress over time. Technologies Backend: Python requests and beautifulsoup for the data collection, Rails for the site Frontend: HTML5, CSS3, dc.js for the home page visualization Info Amazonia Cattle Ranching Description Project developed during the 15th IACC anti-corruption conference hackathon together with Gustavo Faleiros and Carlos Sanchez . It's main goal is to visually show the correlation between cattle ranching growth and its deforestation impact on the Brazilian Amazon. Technologies Backend: Tilemill, GDAL Frontend: Mapbox","tags":"pages","loc":"http://www.juanelosua.com/projects.html","title":"Projects"},{"text":"I enjoy sharing my knowledge and trying to help others being more productive in their projects, can't help that, is in my veins...my mother was an English teacher. I like to make the sessions really hands-on since that is the only way to actually learn things, we learn by doing not by watching others. I have trained some newsroom staff and university students in tools that would help them get started in their data adventures. Scraping tools, OpenRefine, Regular Expressions, Excel methodology and pivot tables sugar, Google Fusion Tables, Infogr.am, DataWrapper even some D3 to the more advanced learners. Here are some organizations where I have been performing training sessions in recent years. Depending on the prior technical knowledge of the audience I can adapt my sessions from basic non-technical tools ( CommandLine...what's that?? ) to more advanced visualization and data wrangling tools. If you think your organization can profit, and believe me it will!!, from a hands-on training session with tools that will help them be more productive working with data, you can contact me through social media or by email","tags":"pages","loc":"http://www.juanelosua.com/training.html","title":"Training sessions"},{"text":"Nicar2015 in Atlanta was my first conference as a Knight-Mozilla fellow and it was a fantastic experience. More than 1,000 people from the journalism & desgin & technology fields gather for an intensive stint of 4-5 days with hands-on sessions, panels and demos on tools to innovate in Computer Assisted Journalism. One of the first things that you realize once you get there is that you are going to have a feeling of missing out no matter how hard you try. There are over 10 simultaneous sessions and unless you can clone yourself (tried that but didn't work...) you need to carefully choose the sessions that you think you can take more profit from. Let me go through some of the sessions that I have enjoyed the most, but before that, I will point you to a really useful resource here curated by @MacDiva for you to dive in the session slides that are more aligned with your field of interest. I can divide the sessions that had more impact on me in two subgroups: technical and management oriented. Here are my favorites: Management oriented sessions Do it once and only once Speakers: Derek Willis & David Eaves Derek and David warned us about the time wasting perils of data processing without automation and audit trails. One of the highlights of the talk to me was finding out about Ben Balter change agent : A Git-backed key-value store, for tracking changes to documents and other files over time as defined by Ben in the repo. I always thought that version control will be applied to other industries outside technology soon, and that is going to be a changemaker in that industry. Slides: Derek Slides & David Slides Processes, standards and documentation for data-driven projects Speakers: Christopher Groskopf & Paul Overberg In this session Paul and Christopher walked us through the importance of creating team standards, project vocabularies and consistent procedures to produce high quality projects in a sustainable way. This is specially crucial when working under a deadline, something that happens naturally in newsrooms. In particular, I found Christopher's part full of wisdom. Coming from someone that has worked in newsrooms and remotely for a while and that has taken the time to share his experience in this invaluable tips . Technical sessions From text to pictures Speakers: Nicholas Diakopoulos On thursday I had a really good start with a session by on how to handle and interpret big amounts of text through visualization by Nicholas Diakopoulos. Nicholas explored different approaches on analizing texts and visualing the content or even the structure to get an insight of what is hidden in that pile of data, after all text is data right? But Nicholas pointed out that text is data but with a particular behavior, the order in which words appear is important to the meaning of a sentence and you need to take that into account. He also talked about the necessary text processing pipeline in order to get a useful analysis Initial text Lowercase Tokenize Stem Stop Word Removal. Slides: Nicholas Slides Plot.ly Speakers: Matthew Sundquist Until NICAR I had not heard about plot.ly (yeah I know...shame on me!!) and I think that it has a lot of potential in the data journalism field and ranging from journalists that want to create graphs without coding to developers through their API . It generates D3.js visualizations under the hood but it exposes many ways to perform an integration with the platform. To find out more just check their tutorials and API documentation Using machine learning to deal with dirty data: a Dedupe demonstration Speakers: Jeff Ernsthausen , Derek Eder , Eric van Zanten & Forest Gregg If I had to pick my favorite session from this year's conference I would have to choose the demo on a tool developed by datamade called Dedupe . It is a machine learning based python library for accurate and scalable data deduplication and entity-resolution. I have fought many times with trying to combine different datasets that do not have a clear identifier to join by. OpenRefine can help with small datasets but the process of clustering is not repeatable and even more important it is not scalable. Having a machine learning process for that kind of task is probably the best way to go. I got so inspired by that demo that I think a big part of my fellowship is going to be focused on machine learning and how to apply it to journalistic problems. As a starter I am going to use dedupe to try to match two different datasets for the upcoming argentinian elections, in next posts I will tell you how it went and what have I learned in the process. Technical tip of the day Let's dig a little bit deeper on how to install Numpy with parallel processing support on a Mac OS X (tested on 10.9 and 10.10) you can read more about the convoluted issue here . Since it took me a while to get things working so I thought maybe sharing my pains can help someone in the future. I will walk you through the installation process that has worked for me, if you think it can be improved don't hesitate to contact me. If you have not installed homebrew, the first step is to do so: $ ruby -e \" $( curl -fsSL https://raw.githubusercontent.com/Homebrew/install/master/install ) \" Set up some compilation flags on your environment of choice export CFLAGS = -Qunused-arguments export CPPFLAGS = -Qunused-arguments Brew tap the homebrew python repo $ brew tap homebrew/python Install Numpy with OpenBlas support $ brew install numpy --with-openblas Check if Numpy is linked against OpenBlas >>> import numpy as np >>> np . __config__ . show () You should see an output like this lapack_opt_info : libraries = [ 'openblas' , 'openblas' ] library_dirs = [ '/usr/local/opt/openblas/lib' ] language = f77 blas_opt_info : libraries = [ 'openblas' , 'openblas' ] library_dirs = [ '/usr/local/opt/openblas/lib' ] language = f77 openblas_info : libraries = [ 'openblas' , 'openblas' ] library_dirs = [ '/usr/local/opt/openblas/lib' ] language = f77 openblas_lapack_info : libraries = [ 'openblas' , 'openblas' ] library_dirs = [ '/usr/local/opt/openblas/lib' ] language = f77 blas_mkl_info : NOT AVAILABLE Now you can continue with the normal installation process for dedupe, just remember to include system packages if you are going to use a virtualenv so that the compiled Numpy with OpenBlas support will be used inside the virtual environment $ virtualenv venv --system-site-packages Wrap up This year's NICAR conference in Atlanta was overall a great experience, It has guided me towards machine learning as a focus for my fellowship year. I think this technique will become more and more adopted by newsrooms in the following years. I have met many people with my same interests (is always good not to feel alone... ;-)). I also had the opportunity to get to chat more with the 2015 knight-mozilla fellows cohort and also spend sometime with other fellows from past years. I would recommend anyone interested in the data journalism field to at least attend once to this conference organized by Investigative Reporters & Editors you will not regret it.","tags":"event","loc":"http://www.juanelosua.com/posts/event/nicar2015/","title":"2015 Nicar conference"},{"text":"2015 is starting and as a new Knight-Mozilla fellows I joined the rest of the cohort in the onboarding event in Los Angeles. I am very grateful to Dan, Erika, Ryan and Erin (in spirit) for giving us the opportunity to get to know the rest of the fellows in more depth both personally and professionally. This fellowship is one of a kind in the sense that each one of us is going to be headed to a different newsroom so this event was a good opportunity to establish relationships that will be kept during 2015 but mainly through remote channels. In this post I will try to share with you my impressions of the event where we have received tons of information and were able to work together in an open source project started by the California Civic Data Coalition that tries to provide a better understanding of campaign finance in the state of California. Day 1 (Jan 12th) To start the onboarding event we went to The Hub LA , Two former fellows Brian Abelson (2013) and Aurelia Moser (2014) joined us and share their experiences and tips with us. It was really important to listen to their advices since they are the best counsellors that we could have having lived the same situations that we are going to have this year. Some of their recommendations were: Try to stay as organized as possible Have a fluent communication about your interests and schedule with your point of contact in the newsroom * Conferences are great but this fellowship is in no way a one-shot opportunity so try to select the ones that will be more profitable. We were also fed with a full plate of information from the opennews staff regarding logistics, policies, community and how to work in the open sharing the experience as one of the main goals of the fellowship. Day 2 (Jan 13th) We visited the LA Times newsroom guided by Ben Welsh . We shamelessly interrupted some journalists and got the chance to ask them questions regarding their work and their relationship with the Data Desk that Ben leads at the newsroom. It was really an interesting visit where we got the sense of the work that each of us will be doing at our host newsroom. Data journalism seems to be a brand new thing but we have learned from people in the LA times that data related investigations have been done since the 80s...of course with different tools but we are not reinventing the wheel, the challenges and workflows of data investigations apply the same now as they did back then. Day 3 (Jan 14th) We went to USC to begin our two hackdays on the CAL-ACCESS campaign browser project. Ben Welsh and Aaron Williams from the CIR gave us an overview of the project goals and the tasks that they had in mind for our contribution to the project. The campaign browser is an ongoing project that aims at helping journalists and other interested persons in improving their way of extracting meaningful information out of the CAL-ACCESS data. We had tasks that were oriented towards frontend as well as backend improvements. Since I am more comfortable in backend tasks, Francis , Ben and I started working on a scraper ran as a custom django-admin command that will complement the information dump that the secretary of state publishes on a daily basis. We wanted to be able to answer some questions that believe it or not were not possible to answer without that scraping task from the original dump, for example: How much money was spent in campaign finance for each election? Also we wanted to be able to link each ballot measure with their supporting/opposing committees and we wrote a second scraper to do that. Tech tip of the day If you have not used Django models before there is one nice feature that I will try to point out when building many-to-many relationships in a Django model. In our example a propostition/ballot measure can have many committees associated with but that works reversely also a committee can support/oppose to many propositions . In relational databases this is known as a many-to-many relationship. Django allows you to model that with a ManyToManyField . When creating the model using the migrate command, Django under the hood will create a table in the Database for the relationship between proposition and committee . $ python manage.py migrate But what if we wanted to characterize that relationship with some attribute as is the case in out example. We don't want just the relationship between a proposition and a committee we want to know also if the commitee is supporting or opposing to the proposition . We can do that in Django by providing a through model to the ManyToManyField (In CAL-ACCESS a committee is identified by a Filer). Now we can explicitly create the relationship model. class Proposition ( BaseModel ): name = models . CharField ( max_length = 255 , null = True ) filer_id_raw = models . IntegerField ( db_index = True ) # election = models . ForeignKey ( Election , null = True , default = None ) filers = models . ManyToManyField ( Filer , through = 'PropositionFiler' ) Take a look at the PropositionFiler class that models the relationship. We need to provide at least the keys that will relate the two models in our case proposition and filer (When not using a through model this would be what django will create under the hood). But now check the position field where we can characterize the relationship with a support or oppose stating whether the committee (filer) opposes/supports the related proposition. class PropositionFiler ( BaseModel ): POSITION_CHOICES = ( ( 'SUPPORT' , 'Support' ), ( 'OPPOSE' , 'Oppose' ), ) proposition = models . ForeignKey ( Proposition ) filer = models . ForeignKey ( Filer ) position = models . CharField ( choices = POSITION_CHOICES , max_length = 50 ) You can use a through model in a many-to-many relationship each time you need to qualify our characterize that relationship with some attributes. You can read more about the through option in the awesome django documentation here Day 4 (Jan 15th) After a successful scrape we cleaned up the code a little bit, document it and pushed the changes. I have not been involved in large open source projects with more than 3 or 4 developers so this was a good opportunity to grasp important ideas related to documentation and workflows needed to keep a distributed open source project running smoothly. Ben, shared with us his vision on the Documentation Driven Development which I thought was a nice advise to push open source projects forward instead of what Dan mentioned to be source available projects. I certainly need to progress on this myself and that is one of the challenges I will be facing during this fellowship year. I switched to some frontend task to help Tara figure out how to handle a visualization that uses dc.js . DC.js is a javascript charting library with native Crossfilter support allowing highly efficient exploration on large multi-dimensional dataset. It leverages D3 engine to render charts in css friendly svg format. If you want to get started with dc.js I found this four part tutorial easy to follow when I try to figure out how to use that library...Trust me give it a chance...you will love it!! Summary Overall the onboarding event was a great experience, we got to work together as a cohort and shared our strong and weak points with the rest. I feel now more comfortable to work together, share our knowledge and collaborate in some projects during this amazing year. To end this post I will like to thank Dan , Erika and Ryan for their support and openness...you make everybody feel welcomed and challenged to keep growing the awesome data journalism tech community out there.","tags":"event","loc":"http://www.juanelosua.com/posts/event/onboarding/","title":"2015 Knight-Mozilla fellowship onboarding"}]}