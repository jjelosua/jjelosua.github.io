{"pages":[{"text":"I am a Spanish Telecommunications engineer with broad experience in tech consultancy and financial services IT. In 2011, I discovered data journalism and became a data addict freelance developer. I love turning data upside down and inside out, trying to extract knowledge out of it. More comfortable with backend tasks, but also a big D3 fan, which I have used to create some nice interactive visualizations. In Spain we did not have a \"freedom of information access\" law until December 2014, so I have been writing scraper scripts to unlock government data for a while, in order to make the information available in a more comprehensive format to citizens. To do so I enjoy working with Python ( Requests and Beautifulsoup ), Ruby ( Mechanize and Nokogiri ) and recently with Phamtomjs to work around ajax sites. I have also trained journalists in Spain and abroad in tools that allow them to find stories and work more efficiently in their data related projects. I strongly believe that open data, transparency and accountability will play a key role in shaping the future of modern societies. When not around my computer you can find me travelling, having a looong conversation with friends or swimming in the ocean. 2015 is going to be a great year!! I was selected as a 2015 Knight-Mozilla fellow ( check out the program here ). I will join La Nacion Data Team to work on cool data journalism projects. This blog has been set up to share my experience throughout this year, please feel free to contact me through social media or by email","tags":"pages","loc":"http://www.juanelosua.com/about.html","title":"About"},{"text":"Selection of the projects that I have worked on over the past couple of years. In most of them I have worked with civio , an spanish non-profit that promotes transparency, accountability and free access to public data to fight for a stronger democracy and empower citizens to have a more proactive role in modern society. España en Llamas (Spanish) Description First Data Journalism project that I have worked on (Hooray!!!). It started with a FOIA request to the spanish government to release the data for every individual forest fires that had occur in Spain from 2001 onwards. After many data wrangling tasks, we achieved a clean database with all the geolocated forest fires with all the available metadata for each one of them. We then created a site that covers some interesting stories we found in the data as well as an interactive map that allows readers to focus on the fires that are more keen to them. Technologies Backend: GIS related tools to transform coordinates, DB transformations Frontend: Google Maps API & D3.js for the interactive map El Indultómetro (Spanish) Description Project also developed in collaboration with civio that compiles, analyzes and classifies all the information in the spanish official gazette about pardons since 1996. Let's readers search by the type of offense, compare annual data and determine how each party in the government has used this controversial measure. Technologies Backend: Ruby mechanize and nokogiri for data collection, Sinatra for the site Frontend: HTML5, CSS3, d3.js, Timeline.js for the visualizations Donde van mis impuestos (Spanish) Description Side project integrated inside a bigger site developed by civio that breaks down spanish budget expenditures. The module compares the regional budget expenditures in Spain from 2006 onwards. Technologies Backend: Ruby gems mechanize & nokogiri to build the scraper Frontend: HTML5, CSS3, D3.js for the visualization Congreso ¿quién es quien? (Spanish) Description Project developed during the 2014 International Open Data Day together with Javier Sanroman and Diego Pino . We gather data for every spanish member of the parliament since modern democracy (1977) together with some metadata related to their activities inside the congress over time. Technologies Backend: Python requests and beautifulsoup for the data collection, Rails for the site Frontend: HTML5, CSS3, dc.js for the home page visualization Info Amazonia Cattle Ranching Description Project developed during the 15th IACC anti-corruption conference hackathon together with Gustavo Faleiros and Carlos Sanchez . It's main goal is to visually show the correlation between cattle ranching growth and its deforestation impact on the Brazilian Amazon. Technologies Backend: Tilemill, GDAL Frontend: Mapbox","tags":"pages","loc":"http://www.juanelosua.com/projects.html","title":"Projects"},{"text":"I enjoy sharing my knowledge and trying to help others being more productive in their projects, can't help that, is in my veins...my mother was an English teacher. I like to make the sessions really hands-on since that is the only way to actually learn things, we learn by doing not by watching others. I have trained some newsroom staff and university students in tools that would help them get started in their data adventures. Scraping tools, OpenRefine, Regular Expressions, Excel methodology and pivot tables sugar, Google Fusion Tables, Infogr.am, DataWrapper even some D3 to the more advanced learners. Here are some organizations where I have been performing training sessions in recent years. Depending on the prior technical knowledge of the audience I can adapt my sessions from basic non-technical tools ( CommandLine...what's that?? ) to more advanced visualization and data wrangling tools. If you think your organization can profit, and believe me it will!!, from a hands-on training session with tools that will help them be more productive working with data, you can contact me through social media or by email","tags":"pages","loc":"http://www.juanelosua.com/training.html","title":"Training sessions"},{"text":"It's shocking to think that half of my fellowship is gone already. It's been five months since I arrived to Buenos Aires and went to La Nación for the first time with butterflies on my stomach, not sure what to expect and how would I feel working in a newsroom. So what's it like to be a Knight-Mozilla fellow? How do you interact with the rest of the newsroom? What's special about being a developer working inside a newsroom? How do you handle time management? In this post I will try to answer some of those questions as well as make some reflections about my fellowship so far. The timeframe to postulate to become a 2016 Knight-Mozilla fellow is open until the 21st of August so I hope this post will help push some of the undecided into submitting their application... you won't regret it. Working in a newsroom as a developer? Impact on society When I joined the data journalism world I was looking for a field that would help me have an impact on society. Working in a newsroom has that potential, most of us get to know what is going on in the outside world through news, whether the information comes from social networks, through traditional newspapers or online sites. This fact comes with two readings: The projects that you develop inside a newsroom are going to hit a big audience, giving you the chance to try to improve those parts of the status quo that are not functioning as they should. Sometimes it allows you to capture attention on an important matter that is buried in a pile of boring official reports that we all have at our disposal. With big audience comes big responsibility. You should work hard to deliver content that is understandable and not geared towards a particular point of view. Thankfully today's tools let you empower readers to interact and explore news apps leveraging some of the pressure from our shoulders. Also, technically, you are pushed to the limit in terms of performance requirements. Multidisciplinary teams I think that interdisciplinary teams are going to become more and more needed for modern enterprises to succeed in this fast-paced ever changing world we are living on. In that sense data journalism blends naturally with that need since the normal team has a variety of profiles ranging from journalists to designers, developers, or even mathematicians or statisticians if you are lucky. The added value that comes out of teams with such different backgrounds is rich when you are careful enough to listen to every member of the team and make them part of the project from the very beginning. One baby each day Even if you are not working on breaking news, as it's my case, the atmosphere that you live has that special rhythm. Not many industries have to deliver a product each day, so the cycle of life is short, thrilling and sometimes stressful, but always exciting. From a developer point of view, working in a deadline based development changes your way of thinking. Most of us are really perfectionists at heart and would like our projects to be state of the art projects that use the best architecture and software pattern designs... But if you have to have it done by 10pm, then some of the out-of-core functionality is just not going to happen and you need to focus on what can be done. Training as a networking tool When embedded in a large newsroom, sometimes it's hard to get to know the rest of the organization and find allies to push projects forward from an editorial point of view. I have used training as an strategy to connect with interested people inside the newsroom and that has helped me feel more integrated at La Nación. I love training, since it helps you clarify your own ideas when you need to get them out of your brain and explain them to someone else. We do not have time to learn all the things we would like so sharing knowledge is a key piece to self-improvement today, both as a trainer or as a trainee. Exclusives vs. OpenSource One of the main interesting conflicts you can live as a fellow is the exclusive vs. opensource battle. Being part of a community driven effort such as Mozilla you are used to and expected to work in the open, sharing code and documentation that would be helpful for the community. Sometimes, the openness we all look forward to, finds an opponent force in the form of news exclusives or reluctances of the newsrooms to share their assets with their competitors. After all newsrooms are private businesses that need to generate revenue if they want to survive. I found timing to be a good way to keep everybody happy. We have developed projects to cover election results in Argentina, and after being published, we have worked on the documentation and code cleaning needed to release them as an open source project. Projects The main reason why I applied for the fellowship was that, even though I had been working in the surroundings of newsrooms, through training and some NGO projects ( Civio ), I wanted to be embedded in an actual newsroom. Therefore, in my case, it was obvious that my main focus would be to work with the rest of the newsroom teams in their projects and try to add value with my skills. Since the begining of my fellowship I have worked in a wide variety of projects inside La Nación: scraping some information out of government pages, adapting a ProPublica timeline tool in order to use it in a storytelling , and last but not the least, I have been working on elections apps... 2015 is a big election year in Argentina. Take a look at La Nacion's github for more information on the projects and the code. Conferences Another part of the fellowship is attending and speaking at conferences. This can be challenging if you are a shy person but it is a needed step in order to get you out of your comfort zone and getting to know other interesting people in the community. This is a list of the conferences I have attended in this five months period: Fellowship on boarding event (Los Angeles - January 2015) NICAR 2015 (Atlanta - March 2015) Jornadas de periodismo de datos OKNF España (Madrid & Barcelona - June 2015) SRCCON (Minneapolis - June 2015) Dato Conference (San Francisco - July 2015) Opennews code convening (Portland - July 2015) Main struggles Let me share with you my main two struggles as a fellow so far. Impostor Syndrome I remember the feeling, when applying to the fellowship after reading previous fellows profiles, I thought I was not qualified to be one of them... but guess what, I was wrong... here I am. Often, what you get from the never ending flow of information we receive each day are finished awesome products. You can feel that if would take you a year to develop those things, and it seems that everybody else can deliver them in a blink of an eye. I was impressed/relieved by my fellowship cohort when we got together for the onboarding in Los Angeles realising that many of us had the same feeling. I guess my point is: you should apply and let Opennews decide if you are qualified or not. They know better!! they want to bring variety to newsrooms and maybe your strengths fit perfectly in what they are searching for a particular candidate newsroom. I still feel that I could be more productive, maybe that is just a part of my perfectionist personality and the continuous learning field I wanted to join. Anyhow, one of the reasons why I left my comfort zone in the banking industry some years ago was that I wanted to be surrounded by people that I admire and I can learn from. The data journalism community has been really welcoming and friendly and has helped me to make that transition. Time management The other main struggle when I look back at these five months is time management. Traveling and attending to conferences bites quite a part of your time as a fellow, working in a newsroom is demanding and sometimes you do not find the time to work on side projects that you are interested in. I decided I needed one day off the newsroom buzz so I can focus on other projects or tasks that I needed to complete. I've talked to my newsroom point of contact, and normally I spend friday's working from home so that I can find the time to do some not-newsroom related work. Summary Being a fellow is just an overall great experience full of excitement, work, travel, conferences and meeting interesting people. You can devote time to dig in those projects that you did not find the time to address but you think will have a great impact on our society. The team at Opennews is just great, they are comprehensive, they give you the freedom to pursue your dream projects and serve as a encyclopedia in terms of connecting you with the right persons to push those projects forward. If you are unsure about what your fellowship experience could be, it's definitely worthwhile to take a look also at what other 2015 fellows experiences have been in their own words: Livia Labate , Francis Tseng and Kavya Sukumar Trust me, you should apply . If you have any doubts, just ping me on twitter and I will be happy to talk through them.","tags":"fellowship","loc":"http://www.juanelosua.com/posts/fellowship/fellowship-midpoint/","title":"Fellowship midpoint"},{"text":"On April 26th Buenos Aires has held its first PASO elections. PASO stands for Simultaneous and Obligatory Open Primary elections. In this election two main things are decided: Which candidate will pass as the official candidate for each political party for the final elections Each political party has to achieve at least 1.5% of the total votes to be able to continue on the election race. These were my first elections inside a newsroom and it was thrilling!! It was like being inside a movie surrounded by journalists waiting for the initial results to rush to their computers to write articles and analysis... but we needed data and I can tell you it was not easy. This 2015 will be a year full of elections: Regional PASO elections, Regional final elections and the mother of all... Presidential elections. In Argentina voting is a compulsory right, so Argentinians will be voting many times during this year. One of the first things that stroke me as a foreigner is that PASO elections are not withheld inside the political party affiliates but rather extended to the whole census. In Spain Primary elections only affect the affiliates and thus are rather overlooked by the overall population. I have had some interesting discussions on whether extending Primary elections to the overall population is a good democratic procedure... on one hand it always seems a good idea to let people decide the candidate that they want for their party of choice...but on the other hand having such a low barrier for making the cut for the next round would theoretically allow \"big\" parties to try to tweak the results of their other contestants by asking their loyal voters to vote for the other party weak candidate...kind of convoluted but possible. The local government decided to switch technological providers for this and upcoming elections and as usual there is some adjustment time to get things working smoothly. At La Nación, we wanted to make a news app that would allow readers to perform realtime analysis of the results of the elections showing as many details as possible, therefore we wanted to show the results by polling station. Frontend isolation Due to the mentioned technical change the local government was running late in providing newsrooms with details on how the results were going to be made available during the realtime vote counting process. So we decided that we needed to blackbox the frontend needs and specify a data format that was required by the frontend leaving the transformation tasks to the backend and assuming that it was going to be doable after all. That way we could move forward in defining the final visualizations and the different reader interaction capabilities and drilldown analysis. Backend nightmare After being really persuasive with government officials, we ended up having some documentation on the 21st of April... we were going to have an API to query but not many details yet. We started defining the backend structure out of thin air, trying to be modular so that we could be efficient when changes were necessary. Trying to divide the functionality in modules helps when you find an error, because you can easily go directly to the module that is causing the error and correct it. But on the other hand, it results in more difficulties in keeping the state of the overall process simple enough. We relied on User-Defined Exceptions to keep track of the overall process while executing code in a module deep down the execution stack. We did not have data yet to test the process so we developed an adhoc simulation script that would randomly produce voting results distributions. My wife to the rescue... Marta helped us develop the results simulation and I believe that was a key decision in the overall success of the project since it allowed us to start testing the backend well ahead of the API availability. Finally we were given an endpoint to test the government API by friday 24th at 18h... fun weekend ahead. Logging: What the h... is going on!!! As a realtime application (running on EC2 machines) we knew that it was almost as important to have an easy way to keep track of what was going on in the backend on realtime to be able to act quickly in case something unexpected happened. We developed a logging strategy that was published each time the process was run (each minute), making it accessible through HTTP. For monitoring purposes we used Postman , a chrome app that lets you define and save collections of HTTP requests: One collection for the government API endpoints One collection for the backend monitoring Concurrency >100.000 simultaneous visitors Being a freelance developer that used to work with NGOs in Spain to increase transparency and the use of Open Data I had not being exposed to the traffic that the homepage of La Nación receives on an election day. So It was really interesting for me to learn from the infrastructure and architecture team the tips & tricks that we needed to take into account in order to be successful: HTTP caches, Content Delivery Networks configuration and static files versioning... All of that was new to me, but really important in terms of performance and bug fixing on the fly. To be able to handle that, we decided that we needed an \"almost\" automatic way of deployment that would decrease the probabilities of human errors while going through a somewhat stressful situation. We used gulp to automate the deployment process, letting it handle the minification, uglyfying and versioning for us. Extract of the gulp deployment process: gulp . task ( 'minify-css' , function () { gulp . src ([ 'css/reset.css' , 'css/fonts.css' , 'css/select2.css' , 'css/style.css' ], { cwd : '../webapp' }) . pipe ( minifyCSS ()) . pipe ( concat ( css_file_min )) . pipe ( gulp . dest ( '../build/css' )); }); gulp . task ( 'test_js' , function (){ return gulp . src ([ 'js/permanentlinkjs.js' , 'js/handlebars_helpers.js' , 'js/elecciones_app.js' , 'js/scripts.js' ], { cwd : '../webapp' }) . pipe ( jshint ()) . pipe ( jshint . reporter ( 'default' )) . pipe ( jshint . reporter ( stylish )); }); gulp . task ( 'js' , [ 'test_js' ], function () { var all = gulp . src ([ 'js/permanentlinkjs.js' , 'js/handlebars_helpers.js' , 'js/elecciones_app.js' , 'js/scripts.js' ] , { cwd : '../webapp' }) . pipe ( sourcemaps . init ()) . pipe ( uglify ()) . pipe ( concat ( js_all )) . pipe ( sourcemaps . write ( './' )) . pipe ( gulp . dest ( '../build/js' )); var vendor = gulp . src ([ 'libs/jquery/dist/jquery.min.js' , 'libs/select2/select2.min.js' , 'libs/handlebars/handlebars.min.js' , 'libs/jquery.nicescroll/jquery.nicescroll.min.js' , ], { cwd : '../webapp' }) . pipe ( sourcemaps . init ()) . pipe ( uglify ()) . pipe ( concat ( js_vendor )) . pipe ( sourcemaps . write ( './' )) . pipe ( gulp . dest ( '../build/libs' )); return merge ( all , vendor ); }); gulp . task ( 'copy' , function () { var opts = { conditionals : true , spare : true }; var html = gulp . src ( '*.html' , { cwd : '../webapp' }) . pipe ( htmlreplace ({ js : [ 'libs/' + js_vendor , 'js/' + js_all ], css : [ 'css/' + css_file_min ] })) . pipe ( minifyHTML ( opts )) . pipe ( gulp . dest ( '../build' )); var fonts = gulp . src ( 'css/fonts/*' , { cwd : '../webapp' }) . pipe ( gulp . dest ( '../build/css/fonts' )); var img = gulp . src ( 'img/*' , { cwd : '../webapp' }) . pipe ( gulp . dest ( '../build/img' )); var css_img = gulp . src ([ 'css/images/*' ], { cwd : '../webapp' }) . pipe ( gulp . dest ( '../build/css/images' )); var data = gulp . src ( 'data/*' , { cwd : '../webapp' }) . pipe ( gulp . dest ( '../build/data' )); return merge ( html , fonts , img , css_img , data ); }); gulp . task ( 'build' , [ 'minify-css' , 'js' , 'copy' ]); Realtime news app At the end, on the election day, we were \"anxious\" to check if the kind of creative way of connecting the dots was going to work out... and it did!! After sometime while the API was not responding we were able to get a full response to our backend and transform it to the frontend expected format... that was at 22:30 Sunday night... we even published the initial results before the official government page that was suffering from availability issues. We have cleaned up after ourselves and have released the code of the project on github here Calm after the storm... Nope!! After the adrenaline came down a bit we realized that had not completed our task since we wanted to let the readers go deep into their local details... we needed the results by polling station. Luckily Manuel Aristarán had worked on a similar project for the 2013 elections and we had worked in advance to obtain the geolocation of each polling station. Thanks to that we could delivered a map that showed the results for each polling station the same day we got the official results from the Buenos Aires Government. We used cartodb.js API require.js in order to keep the javascript development modular. To avoid having too many HTTP requests we finally integrated the requirejs optimizer into the gulp deployment pipeline so that we have the benefit of modularity in development but with a nice performance on production. Here is the open sourced code for that map. Wrap up I am really grateful for being able to have lived the election experience inside a newsroom. We have worked hard as a team and the overall results were really positive... really proud of the team and the effort. Thanks to the rest of the team Cristian Bertelegni , Gaston de la Llana and Pablo Loscri for keeping such a nice working atmosphere. I would also like to send special thanks to Flor Fernández and Mariana Trigo for their support. It is worth mentioning that all this work and analysis was not only used in the online edition at La Nación. The results from the developed News Apps were also used to enrich the reader experience on the paper edition. That means a double win for the newsroom: creating an interactive application that will also generate value on the printed version.","tags":"fellowship","loc":"http://www.juanelosua.com/posts/fellowship/2015pasocaba/","title":"2015 PASO CABA Elections"},{"text":"Nicar2015 in Atlanta was my first conference as a Knight-Mozilla fellow and it was a fantastic experience. More than 1,000 people from the journalism & desgin & technology fields gather for an intensive stint of 4-5 days with hands-on sessions, panels and demos on tools to innovate in Computer Assisted Journalism. One of the first things that you realize once you get there is that you are going to have a feeling of missing out no matter how hard you try. There are over 10 simultaneous sessions and unless you can clone yourself (tried that but didn't work...) you need to carefully choose the sessions that you think you can take more profit from. Let me go through some of the sessions that I have enjoyed the most, but before that, I will point you to a really useful resource here curated by @MacDiva for you to dive in the session slides that are more aligned with your field of interest. I can divide the sessions that had more impact on me in two subgroups: technical and management oriented. Here are my favorites: Management oriented sessions Do it once and only once Speakers: Derek Willis & David Eaves Derek and David warned us about the time wasting perils of data processing without automation and audit trails. One of the highlights of the talk to me was finding out about Ben Balter change agent : A Git-backed key-value store, for tracking changes to documents and other files over time as defined by Ben in the repo. I always thought that version control will be applied to other industries outside technology soon, and that is going to be a changemaker in that industry. Slides: Derek Slides & David Slides Processes, standards and documentation for data-driven projects Speakers: Christopher Groskopf & Paul Overberg In this session Paul and Christopher walked us through the importance of creating team standards, project vocabularies and consistent procedures to produce high quality projects in a sustainable way. This is specially crucial when working under a deadline, something that happens naturally in newsrooms. In particular, I found Christopher's part full of wisdom. Coming from someone that has worked in newsrooms and remotely for a while and that has taken the time to share his experience in this invaluable tips . Technical sessions From text to pictures Speakers: Nicholas Diakopoulos On thursday I had a really good start with a session on how to handle and interpret big amounts of text through visualization by Nicholas Diakopoulos. Nicholas explored different approaches on analizing texts and visualing the content or even the structure to get an insight of what is hidden in that pile of data, after all text is data right? But Nicholas pointed out that text is data but with a particular behavior, the order in which words appear is important to the meaning of a sentence and you need to take that into account. He also talked about the necessary text processing pipeline in order to get a useful analysis Initial text Lowercase Tokenize Stem Stop Word Removal. Slides: Nicholas Slides Plot.ly Speakers: Matthew Sundquist Until NICAR I had not heard about plot.ly (yeah I know...shame on me!!) and I think that it has a lot of potential in the data journalism field and ranging from journalists that want to create graphs without coding to developers through their API . It generates D3.js visualizations under the hood but it exposes many ways to perform an integration with the platform. To find out more just check their tutorials and API documentation Using machine learning to deal with dirty data: a Dedupe demonstration Speakers: Jeff Ernsthausen , Derek Eder , Eric van Zanten & Forest Gregg If I had to pick my favorite session from this year's conference I would have to choose the demo on a tool developed by datamade called Dedupe . It is a machine learning based python library for accurate and scalable data deduplication and entity-resolution. I have fought many times with trying to combine different datasets that do not have a clear identifier to join by. OpenRefine can help with small datasets but the process of clustering is not repeatable and even more important it is not scalable. Having a machine learning process for that kind of task is probably the best way to go. I got so inspired by that demo that I think a big part of my fellowship is going to be focused on machine learning and how to apply it to journalistic problems. As a starter I am going to use dedupe to try to match two different datasets for the upcoming argentinian elections, in next posts I will tell you how it went and what have I learned in the process. Technical tip of the day Let's dig a little bit deeper on how to install Numpy with parallel processing support on a Mac OS X (tested on 10.9 and 10.10) you can read more about the convoluted issue here . Since it took me a while to get things working so I thought maybe sharing my pains can help someone in the future. I will walk you through the installation process that has worked for me, if you think it can be improved don't hesitate to contact me. If you have not installed homebrew, the first step is to do so: $ ruby -e \" $( curl -fsSL https://raw.githubusercontent.com/Homebrew/install/master/install ) \" Set up some compilation flags on your environment of choice export CFLAGS = -Qunused-arguments export CPPFLAGS = -Qunused-arguments Brew tap the homebrew python repo $ brew tap homebrew/python Install Numpy with OpenBlas support $ brew install numpy --with-openblas Check if Numpy is linked against OpenBlas >>> import numpy as np >>> np . __config__ . show () You should see an output like this lapack_opt_info : libraries = [ 'openblas' , 'openblas' ] library_dirs = [ '/usr/local/opt/openblas/lib' ] language = f77 blas_opt_info : libraries = [ 'openblas' , 'openblas' ] library_dirs = [ '/usr/local/opt/openblas/lib' ] language = f77 openblas_info : libraries = [ 'openblas' , 'openblas' ] library_dirs = [ '/usr/local/opt/openblas/lib' ] language = f77 openblas_lapack_info : libraries = [ 'openblas' , 'openblas' ] library_dirs = [ '/usr/local/opt/openblas/lib' ] language = f77 blas_mkl_info : NOT AVAILABLE Now you can continue with the normal installation process for dedupe, just remember to include system packages if you are going to use a virtualenv so that the compiled Numpy with OpenBlas support will be used inside the virtual environment $ virtualenv venv --system-site-packages Wrap up This year's NICAR conference in Atlanta was overall a great experience, It has guided me towards machine learning as a focus for my fellowship year. I think this technique will become more and more adopted by newsrooms in the following years. I have met many people with my same interests (is always good not to feel alone... ;-)). I also had the opportunity to get to chat more with the 2015 knight-mozilla fellows cohort and also spend sometime with other fellows from past years. I would recommend anyone interested in the data journalism field to at least attend once to this conference organized by Investigative Reporters & Editors you will not regret it.","tags":"event","loc":"http://www.juanelosua.com/posts/event/nicar2015/","title":"2015 Nicar conference"},{"text":"2015 is starting and as a new Knight-Mozilla fellows I joined the rest of the cohort in the onboarding event in Los Angeles. I am very grateful to Dan, Erika, Ryan and Erin (in spirit) for giving us the opportunity to get to know the rest of the fellows in more depth both personally and professionally. This fellowship is one of a kind in the sense that each one of us is going to be headed to a different newsroom so this event was a good opportunity to establish relationships that will be kept during 2015 but mainly through remote channels. In this post I will try to share with you my impressions of the event where we have received tons of information and were able to work together in an open source project started by the California Civic Data Coalition that tries to provide a better understanding of campaign finance in the state of California. Day 1 (Jan 12th) To start the onboarding event we went to The Hub LA , Two former fellows Brian Abelson (2013) and Aurelia Moser (2014) joined us and share their experiences and tips with us. It was really important to listen to their advices since they are the best counsellors that we could have having lived the same situations that we are going to have this year. Some of their recommendations were: Try to stay as organized as possible Have a fluent communication about your interests and schedule with your point of contact in the newsroom * Conferences are great but this fellowship is in no way a one-shot opportunity so try to select the ones that will be more profitable. We were also fed with a full plate of information from the opennews staff regarding logistics, policies, community and how to work in the open sharing the experience as one of the main goals of the fellowship. Day 2 (Jan 13th) We visited the LA Times newsroom guided by Ben Welsh . We shamelessly interrupted some journalists and got the chance to ask them questions regarding their work and their relationship with the Data Desk that Ben leads at the newsroom. It was really an interesting visit where we got the sense of the work that each of us will be doing at our host newsroom. Data journalism seems to be a brand new thing but we have learned from people in the LA times that data related investigations have been done since the 80s...of course with different tools but we are not reinventing the wheel, the challenges and workflows of data investigations apply the same now as they did back then. Day 3 (Jan 14th) We went to USC to begin our two hackdays on the CAL-ACCESS campaign browser project. Ben Welsh and Aaron Williams from the CIR gave us an overview of the project goals and the tasks that they had in mind for our contribution to the project. The campaign browser is an ongoing project that aims at helping journalists and other interested persons in improving their way of extracting meaningful information out of the CAL-ACCESS data. We had tasks that were oriented towards frontend as well as backend improvements. Since I am more comfortable in backend tasks, Francis , Ben and I started working on a scraper ran as a custom django-admin command that will complement the information dump that the secretary of state publishes on a daily basis. We wanted to be able to answer some questions that believe it or not were not possible to answer without that scraping task from the original dump, for example: How much money was spent in campaign finance for each election? Also we wanted to be able to link each ballot measure with their supporting/opposing committees and we wrote a second scraper to do that. Tech tip of the day If you have not used Django models before there is one nice feature that I will try to point out when building many-to-many relationships in a Django model. In our example a propostition/ballot measure can have many committees associated with but that works reversely also a committee can support/oppose to many propositions . In relational databases this is known as a many-to-many relationship. Django allows you to model that with a ManyToManyField . When creating the model using the migrate command, Django under the hood will create a table in the Database for the relationship between proposition and committee . $ python manage.py migrate But what if we wanted to characterize that relationship with some attribute as is the case in out example. We don't want just the relationship between a proposition and a committee we want to know also if the commitee is supporting or opposing to the proposition . We can do that in Django by providing a through model to the ManyToManyField (In CAL-ACCESS a committee is identified by a Filer). Now we can explicitly create the relationship model. class Proposition ( BaseModel ): name = models . CharField ( max_length = 255 , null = True ) filer_id_raw = models . IntegerField ( db_index = True ) # election = models . ForeignKey ( Election , null = True , default = None ) filers = models . ManyToManyField ( Filer , through = 'PropositionFiler' ) Take a look at the PropositionFiler class that models the relationship. We need to provide at least the keys that will relate the two models in our case proposition and filer (When not using a through model this would be what django will create under the hood). But now check the position field where we can characterize the relationship with a support or oppose stating whether the committee (filer) opposes/supports the related proposition. class PropositionFiler ( BaseModel ): POSITION_CHOICES = ( ( 'SUPPORT' , 'Support' ), ( 'OPPOSE' , 'Oppose' ), ) proposition = models . ForeignKey ( Proposition ) filer = models . ForeignKey ( Filer ) position = models . CharField ( choices = POSITION_CHOICES , max_length = 50 ) You can use a through model in a many-to-many relationship each time you need to qualify our characterize that relationship with some attributes. You can read more about the through option in the awesome django documentation here Day 4 (Jan 15th) After a successful scrape we cleaned up the code a little bit, document it and pushed the changes. I have not been involved in large open source projects with more than 3 or 4 developers so this was a good opportunity to grasp important ideas related to documentation and workflows needed to keep a distributed open source project running smoothly. Ben, shared with us his vision on the Documentation Driven Development which I thought was a nice advise to push open source projects forward instead of what Dan mentioned to be source available projects. I certainly need to progress on this myself and that is one of the challenges I will be facing during this fellowship year. I switched to some frontend task to help Tara figure out how to handle a visualization that uses dc.js . DC.js is a javascript charting library with native Crossfilter support allowing highly efficient exploration on large multi-dimensional dataset. It leverages D3 engine to render charts in css friendly svg format. If you want to get started with dc.js I found this four part tutorial easy to follow when I try to figure out how to use that library...Trust me give it a chance...you will love it!! Summary Overall the onboarding event was a great experience, we got to work together as a cohort and shared our strong and weak points with the rest. I feel now more comfortable to work together, share our knowledge and collaborate in some projects during this amazing year. To end this post I will like to thank Dan , Erika and Ryan for their support and openness...you make everybody feel welcomed and challenged to keep growing the awesome data journalism tech community out there.","tags":"event","loc":"http://www.juanelosua.com/posts/event/onboarding/","title":"2015 Knight-Mozilla fellowship onboarding"}]}